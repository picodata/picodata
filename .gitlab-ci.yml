default:
  tags:
    - docker-picodata
  image:
    name: ${BASE_IMAGE}:${BASE_IMAGE_TAG}
    pull_policy: always
  interruptible: true
  retry:
    max: 1
    when:
      - scheduler_failure
      - runner_system_failure
      - stuck_or_timeout_failure
      - api_failure

stages:
  - build-base-image
  - clone
  - test
  - pack
  - docker
  - build-stress-image
  - stress-test
  - deploy

workflow:
  # See https://docs.gitlab.com/ee/ci/jobs/job_control.html#avoid-duplicate-pipelines
  rules:
    # To avoid duplicate pipelines we disable merge request events,
    # leaving only pushes and manual triggering.
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: never
    - if: $CI_PIPELINE_SOURCE == "push"
    - if: $CI_PIPELINE_SOURCE == "web"
    - if: $CI_PIPELINE_SOURCE == "schedule"

variables:
  REGISTRY: docker-public.binary.picodata.io
  BASE_IMAGE: ${CI_REGISTRY_IMAGE}/picodata-build-base
  BASE_IMAGE_DOCS: ${CI_REGISTRY_IMAGE}/docs
  BASE_IMAGE_LATEST: latest
  BASE_IMAGE_TAG: ${BASE_IMAGE_LATEST}
  MAIN_BRANCH: &main-branch master
  CARGO_HOME: $CI_PROJECT_DIR/.cargo
  KANIKO_REGISTRY_MIRROR: docker-proxy.binary.picodata.io
  FF_USE_FASTZIP: "true"
  FF_TIMESTAMPS: "true"
  CACHE_COMPRESSION_LEVEL: "fastest"
  CACHE_POLICY: pull-push
  GIT_STRATEGY: clone
  GIT_DEPTH: 1
  GET_SOURCES_ATTEMPTS: 3
  GIT_FETCH_TAGS_DIRS: ".,tarantool-sys,tarantool-sys/third_party/luajit"
  PICODATA_DESCRIBE: "25.4.0"
  PARENT_BRANCH: $CI_COMMIT_REF_NAME
  PARENT_CI_COMMIT_SHA: $CI_COMMIT_SHA
  PARENT_PROJECT_PATH: ${CI_PROJECT_PATH}.git
  TARANTOOL_VERSION: 2.11.2.159
  RUST_VERSION: 1.85
  FF_NETWORK_PER_BUILD: 1
  RAW_REGISTRY: $RAW_PRIVATE_REGISTRY
  CI_DEBUG_SERVICES: "true"
  # k8s runner config
  KUBERNETES_CPU_REQUEST: 2
  KUBERNETES_MEMORY_REQUEST: "4Gi"
  SCCACHE_DIR: $CI_PROJECT_DIR/.sccache
  SCCACHE_CACHE_SIZE: "15G"
  COMPILER_LAUNCHER: "sccache"

# job:rules explained:
#
# - if build-base changes on master branch (compared to HEAD~1)
#     * build-base-image (with tag latest) and push
#     * test (on base-image:latest)
# - if build-base changes on development branch (compared to master)
#     * build-base-image (with tag sha)
#     * test (on base-image:sha)
# - else (if build-base doesn't change)
#     * skip build-base-image
#     * just test (on base-image:latest)
#
# Anchor syntax explained here:
# https://docs.gitlab.com/ee/ci/yaml/yaml_optimization.html
#
.rules:
  - &if-build-base-changes-on-master-branch
    if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
    changes:
      paths: &build-base-changes-paths
        - docker-build-base/**
        - tools/clone.sh
        - .gitlab-ci.yml

  - &if-build-base-changes-on-dev-branch
    if: ($CI_COMMIT_BRANCH != $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
    changes:
      paths:
        - docker-build-base/**
        - tools/clone.sh
        - .gitlab-ci.yml
      compare_to: *main-branch

  - &if-changes-on-docs
    if: $CI_PIPELINE_SOURCE != "schedule"
    changes:
      paths:
        - docs/**/*
      compare_to: *main-branch

  - &if-no-docs
    if: ($CI_PIPELINE_SOURCE != "schedule") && ($CI_COMMIT_BRANCH != $MAIN_BRANCH) && ($CI_COMMIT_BRANCH !~ /^docs/)

.cache_git: &cache_git
  paths:
    - $CI_PROJECT_DIR
  key: cache_git-$CI_COMMIT_BRANCH
  fallback_keys:
    - cache_git-$MAIN_BRANCH

.base_cache: &base_cache
  paths:
    - .cargo/
    - .sccache/
  key: "base_cache-${TARGET}-$CI_COMMIT_REF_SLUG"
  fallback_keys:
    - base_cache-${TARGET}-$MAIN_BRANCH

.base_node: &base_node
  paths:
    - webui/node_modules/
  key: "base_node-$CI_COMMIT_REF_SLUG"
  fallback_keys:
    - base_node-$MAIN_BRANCH

.py_cache: &py_cache
  paths:
    - .venv
  key: "py_cache-$CI_COMMIT_REF_SLUG"
  fallback_keys:
    - py_cache-$MAIN_BRANCH

.kaniko_image: &kaniko_image
  image:
    name: docker-public.binary.picodata.io/kaniko-project/executor:v1.23.1-debug
    entrypoint: [""]
    pull_policy: [if-not-present]
  tags:
    - docker-k8s
  variables:
    GIT_USERNAME: $CI_REGISTRY_USER
    GIT_PASSWORD: $CI_REGISTRY_PASSWORD
  script:
    - >
      /kaniko/executor --context $CI_PROJECT_DIR --dockerfile ${DOCKERFILE}
      --build-arg TARANTOOL_VERSION=${TARANTOOL_VERSION}
      --build-arg RUST_VERSION=${RUST_VERSION}
      --build-arg BASE_IMAGE=${BASE_IMAGE}
      --build-arg BASE_IMAGE_TAG=${BASE_IMAGE_TAG}
      ${PUSH_DOCKER}
      --cache=false --cache-run-layers=true --single-snapshot --compressed-caching=false --use-new-run --snapshot-mode=redo --cleanup
      --destination ${DESTINATION}

build-base-image:
  stage: build-base-image
  <<: *kaniko_image
  rules:
    - <<: *if-build-base-changes-on-master-branch
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${PARENT_CI_COMMIT_SHA}
    - when: manual
      allow_failure: true
      variables:
        BASE_IMAGE_TAG: ${BASE_IMAGE_LATEST}
  variables:
    DOCKERFILE: docker-build-base/Dockerfile
    DESTINATION: ${BASE_IMAGE}:${BASE_IMAGE_TAG}

build-docs-image:
  stage: build-base-image
  <<: *kaniko_image
  rules:
    - if: $CI_PIPELINE_SOURCE != "schedule"
      changes:
        paths:
          - docker-build-base/docs.Dockerfile
          - .gitlab-ci.yml
        compare_to: *main-branch
      variables:
        BASE_IMAGE_TAG: ${PARENT_CI_COMMIT_SHA}
    - when: manual
      allow_failure: true
      variables:
        BASE_IMAGE_TAG: ${BASE_IMAGE_LATEST}
  variables:
    DOCKERFILE: docker-build-base/docs.Dockerfile
    DESTINATION: ${BASE_IMAGE_DOCS}:${BASE_IMAGE_TAG}

build-stress-image:
  stage: build-stress-image
  <<: *kaniko_image
  rules:
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${PARENT_CI_COMMIT_SHA}
      needs: ["build-base-image"]

    - when: manual
      allow_failure: true
  variables:
    GIT_SUBMODULE_STRATEGY: recursive
    DOCKERFILE: docker-build-base/stress.Dockerfile
    DESTINATION: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA

# When on master, the job creates cache containing fully fetched repository in working directory.
# The cache includes submodules. This will happen once
# per day or more. In dev branches, the cache is taken from the master by
# fallback_keys and the current commit is fetched. In dev branches cloning via
# gitlab is disabled in tests and lint, and implemented via git clone/fetch.
clone:
  stage: clone
  cache:
    - <<: *cache_git
      policy: $CACHE_POLICY
  rules:
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
      variables:
     # $MAIN_BRANCH should use a push cache policy so that master doesn't pull
     # the previous master. This is important because there may be uncommitted
     # changes when using git fetch -uf
        CACHE_POLICY: push
     # dev branches should use a pull-push cache policy so that tests have the
     # corresponding dev branch that we fetched during the clone step. This is
     # important because tests should check the dev branch, not the master.
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
    - <<: *if-no-docs
    - if: $CI_PIPELINE_SOURCE == "schedule"
  variables:
    GIT_STRATEGY: none
    # slow is as fast as the fastest level, but the cache takes up 40% less space
    CACHE_COMPRESSION_LEVEL: "slow"
  script:
    - clone.sh
    # Gitlab CI implicitly clones specific refs (e.g. `refs/pipelines/xxxxxxx`),
    # but it doesn't imply fetching tags. We clone them manually with the
    # `git fetch` command.
    #
    # Tags in `tarantool-sys` and `luajit` submodules are necessary for
    # the build scripts. Without them the job fails.
    - ./tools/get_tags.py

.test:
  stage: test
  variables:
    GIT_DESCRIBE: $PICODATA_DESCRIBE
    RUST_BACKTRACE: full
    GIT_STRATEGY: none
    KUBERNETES_CPU_REQUEST: 6
    KUBERNETES_MEMORY_REQUEST: "6Gi"
    # By default "fast-release" profile is used, because of its faster
    # compilation time compared to the "release" profile, which includes
    # additional optimizations.
    # The actual "release" profile is only used when running on master branch.
  rules:
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
      variables:
        RELEASE_PROFILE: &release-profile "release"
        RELEASE_TARGET: &release-target "release"
        # if the cache policy is pull-push, then the cache is layered on the previous version of the cache, and it takes up more and more space
        CACHE_POLICY: push
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
        RELEASE_PROFILE: &release-profile "fast-release"
        RELEASE_TARGET: &release-target "fast-release"
        CACHE_POLICY: pull
    - <<: *if-no-docs
      variables:
        RELEASE_PROFILE: &release-profile "fast-release"
        RELEASE_TARGET: &release-target "fast-release"
        CACHE_POLICY: pull

.parallel:
  parallel:
    matrix:
      - BUILD_PROFILE: *release-profile
        TARGET: *release-target
      - BUILD_PROFILE: dev
        TARGET: debug

.poetry-install: &poetry-install |
  # Poetry install
  ci-log-section start "poetry-install" Installing python dependencies ...
  POETRY_VIRTUALENVS_IN_PROJECT=1 PIP_NO_CACHE_DIR=true poetry install --no-interaction --ansi
  ci-log-section end "poetry-install"

test-linux:
  extends:
    - .test
    - .parallel
  cache:
    - <<: *cache_git
      policy: pull
    - <<: *py_cache
      policy: $CACHE_POLICY
    - <<: *base_cache
      policy: $CACHE_POLICY
    - <<: *base_node
      policy: $CACHE_POLICY
  script:
    - git rev-parse HEAD
    - tools/check_rust.sh
    - cargo -V

    # Setup sccache
    - |
      ci-log-section start "sccache-setup" Setting up sccache ...
      sccache --show-stats
      ci-log-section end "sccache-setup"

    - *poetry-install
    - pushd sbroad && make test
    - make bench_check; popd

    - |
      ci-log-section start "build" Build
      make build-$BUILD_PROFILE CARGO_FLAGS_EXTRA="--timings"
      ci-log-section end "build"

    # Show sccache statistics
    - |
      ci-log-section start "sccache-stats" sccache statistics ...
      sccache --show-stats
      ci-log-section end "sccache-stats"
    - |
      ci-log-section start "test" Test
      make test \
        PYTEST_FLAGS="--junitxml=junit_pytest.xml --with-webui -n$KUBERNETES_CPU_REQUEST --durations=10" \
        CARGO_FLAGS_EXTRA="--profile=$BUILD_PROFILE"
      ci-log-section end "test"

  artifacts:
    when: always
    paths:
      - junit_pytest.xml
      - ./target/cargo-timings/cargo-timing.html
      - core*
      - target/$TARGET/picodata
    reports:
      junit: junit_pytest.xml

lint:
  stage: test
  rules:
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
        CACHE_POLICY: pull
    - <<: *if-no-docs
      variables:
        CACHE_POLICY: pull
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
  variables:
    GIT_STRATEGY: none
    GIT_DESCRIBE: $PICODATA_DESCRIBE
    TARGET: debug
  cache:
    - <<: *cache_git
      policy: pull
    - <<: *py_cache
      policy: pull
    - <<: *base_cache
      policy: pull
    - <<: *base_node
      policy: pull
  script:
    # Setup sccache
    - |
      ci-log-section start "sccache-setup" Setting up sccache ...
      sccache --show-stats
      ci-log-section end "sccache-setup"

    - *poetry-install
    - make lint
    - pushd webui && yarn lint; popd
    - pushd sbroad && make check

    # Show sccache statistics
    - |
      ci-log-section start "sccache-stats" sccache statistics ...
      sccache --show-stats
      ci-log-section end "sccache-stats"

lint-docs:
  image:
    name: ${BASE_IMAGE_DOCS}:${BASE_IMAGE_TAG}
    pull_policy: always
  stage: test
  tags:
    - docker-k8s
  rules:
    - <<: *if-changes-on-docs
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
  script:
    - echo "Checking picodata doc..."
    - pushd docs && pipenv run lint
    - echo "Picodata doc successfully checked"

.test-patch-rules: &test-patch-rules
  rules:
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE == "schedule")
    - <<: *if-build-base-changes-on-dev-branch
      when: manual
      allow_failure: true
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
    - when: manual
      allow_failure: true

pack-doc:
  image:
    name: ${BASE_IMAGE_DOCS}:${BASE_IMAGE_TAG}
    pull_policy: always
  stage: pack
  tags:
    - docker-k8s
  needs: [lint-docs]
  rules:
    - <<: *if-changes-on-docs
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
  variables:
    SED_SLUG: "s/[^a-zA-Z0-9.]+/_/g"
  before_script:
    - |
      if [ "$CI_COMMIT_BRANCH" == "master" ]; then
        PICODATA_DOC_VER="devel"
      elif [ "$CI_COMMIT_BRANCH" == "$MAIN_BRANCH" ]; then
        PICODATA_DOC_VER="$MAIN_BRANCH"
      else
        PICODATA_DOC_VER="$(echo $CI_COMMIT_REF_NAME | sed -E "$SED_SLUG")"
      fi
    - |
      if [ "$CI_COMMIT_BRANCH" == "$MAIN_BRANCH" ]; then
        SITE_URL="https://docs.picodata.io/picodata/$PICODATA_DOC_VER"
      else
        SITE_URL="https://docs.binary.picodata.io/picodata/$PICODATA_DOC_VER"
      fi
    - cd docs/
  script:
    # Automatic Loading of .env — https://pipenv.pypa.io/en/stable/shell.html
    - echo "Pack picodata doc..."
    - echo "PICODATA_DOC_VER=$PICODATA_DOC_VER" | tee .env
    - echo "SITE_URL=$SITE_URL" | tee -a .env
    - DATE=$(date +%Y%m%d%H%M%S)
    - FNAME="picodata-doc-${DATE}-${PICODATA_DOC_VER}-${CI_COMMIT_SHA}.tgz"
    - echo "FNAME=$FNAME" | tee -a .env
    - echo "GIT_DESCRIBE=$(git describe --always)" | tee -a .env
    - pipenv run mkdocs build -sd build
    - pushd build
    - tar -cvzf ../$FNAME .
    - popd
    - echo "Picodata doc successfully packed."
  artifacts:
    name: "artifacts-vars-${CI_PIPELINE_ID}"
    paths:
      - docs/picodata-doc-*.tgz
    reports:
      dotenv: docs/.env

upload-doc-to-binary:
  image: docker.binary.picodata.io/curlimages/curl
  stage: pack
  tags:
    - docker-k8s
  rules:
    - <<: *if-changes-on-docs
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
  variables:
    GIT_STRATEGY: none
  script:
    - cd docs/
    - echo "Upload picodata doc to binary..."
    - echo "FNAME=$FNAME"
    - 'curl --fail -H "Authorization: Basic ${WWW_RAW_RW}" --upload-file $FNAME https://binary.picodata.io/repository/www-raw/$FNAME'
    - echo "Picodata doc successfully uploaded to binary."
  needs:
    - job: pack-doc
      artifacts: true

.deploy-doc:
  stage: deploy
  rules:
    - <<: *if-changes-on-docs
  variables:
    STAGE: TEST
    PICODATA_DOC: $FNAME
    PICODATA_DOC_SUBDIR: $PICODATA_DOC_VER
  trigger:
    project: "picodata/web-site/infra"
    strategy: depend
  needs:
    - job: pack-doc
      artifacts: true
    - job: upload-doc-to-binary

deploy-doc-staging:
  extends:
    - .deploy-doc

deploy-doc-prod:
  extends:
    - .deploy-doc
  rules:
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
  variables:
    STAGE: PROD

test-patch-picodata:
  extends: .test
  <<: *test-patch-rules
  variables:
    BUILD_PROFILE: release
    KUBERNETES_CPU_REQUEST: 8
    KUBERNETES_MEMORY_REQUEST: "8Gi"
  cache:
    - <<: *py_cache
      policy: pull
    - <<: *base_cache
      policy: pull
    - <<: *base_node
      policy: pull
  script:
    - *poetry-install
    - ./tools/prepare_source_tree_for_stat_analysis.py apply
    - make build-$BUILD_PROFILE CARGO_FLAGS_EXTRA="--features webui,dynamic_build"
    - |
      make test \
        PYTEST_FLAGS="--junitxml=junit_pytest.xml --with-webui" \
        CARGO_FLAGS_EXTRA="--profile=$BUILD_PROFILE --features dynamic_build"

test-patch-tarantool:
  extends: .test
  <<: *test-patch-rules
  script:
    - ./tools/prepare_source_tree_for_stat_analysis.py apply
    - pushd tarantool-sys/ && make -f .test.mk test-release;
  after_script:
    - find /tmp/t/ -type s -delete
    - mv /tmp/t tarantool-sys/test/artifacts
  artifacts:
    when: always
    paths:
      - tarantool-sys/test/artifacts/

test-mac-m1:
  extends: .test
  variables:
    GIT_STRATEGY: fetch
  rules:
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
    - when: manual
      allow_failure: true
  tags:
    - mac-dev-m1
  timeout: 60m
  script:
    # Gitlab doesnt know about $HOME. If specified in variables it becomes empty
    - export CARGO_HOME=$HOME/.cargo
    - ./tools/get_tags.py
    - cargo +$RUST_VERSION -V
    - cargo +$RUST_VERSION build --locked --timings
    - cp target/cargo-timings/cargo-timing.html cargo-timing-without-webui.html
    - cargo +$RUST_VERSION build --features webui --locked --timings
    - cp target/cargo-timings/cargo-timing.html cargo-timing-with-webui.html
    # There are no Rust tests for `webui` feature.
    # It will be checked during integration tests.
    - cargo +$RUST_VERSION test --locked

    - cargo +$RUST_VERSION fmt -- -v --check
    - cargo +$RUST_VERSION clippy --version
    - cargo +$RUST_VERSION clippy --features "load_test webui error_injection" -- --deny clippy::all --no-deps
    # - *poetry-install
    # - poetry run pytest --numprocesses auto -v
    # - make lint
    # - |
  artifacts:
    when: always
    paths:
      - cargo-timing-without-webui.html
      - cargo-timing-with-webui.html
      - target/debug/picodata

.helm:
  stage: test
  needs: []
  rules:
    - if: $CI_COMMIT_BRANCH != $MAIN_BRANCH
      changes:
        compare_to: *main-branch
        paths:
          - docker/picodata.Dockerfile
          - docker/picodata-distroless.Dockerfile
          - docker/docker-compose.yml
      variables:
        PUSH_DOCKER: "--no-push"
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE == "schedule")
      when: on_success
      variables:
        PUSH_DOCKER: ""
    - when: manual
      allow_failure: true

helm-image:
  extends: .helm
  variables:
    GIT_STRATEGY: none
    KUBERNETES_CPU_REQUEST: 8
    KUBERNETES_MEMORY_REQUEST: "8Gi"
    MULTIARCH: "true"
  parallel:
    matrix:
      - DOCKERFILE: docker/picodata.Dockerfile
        DESTINATION: ${REGISTRY}/picodata:master
      - DOCKERFILE: docker/picodata-distroless.Dockerfile
        DESTINATION: ${REGISTRY}/picodata:master-distroless
  trigger:
    project: picodata/devops/picodata-in-docker
    branch: main
    strategy: depend

gamayun-prepare:
  extends: .test
  variables:
      GIT_STRATEGY: clone
  when: manual
  rules:
    - <<: *if-build-base-changes-on-dev-branch
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
    - when: manual
  script:
    - export CARGO_HOME=$HOME/.cargo
    - cargo -V
    - cargo clippy --locked --message-format=json > clippy.json
  artifacts:
    when: always
    paths:
      - clippy.json

gamayun-run:
  extends: .test
  variables:
      GIT_STRATEGY: clone
  needs: ["gamayun-prepare"]
  tags:
    - picodata-shell
  script:
    # create an ssh tunnel to gamayun server to allow the report uploading
    - TUNNEL="ssh -4 -L 9000:localhost:9000 sonar-reports -N -f"
    - eval $TUNNEL
    - ls -la
    - |
      # svace patches is more iportant than gamayun and order of applying is important
      # first - svace, second - gamayun
      ./tools/prepare_source_tree_for_stat_analysis.py apply
    - |
      # TODO consider moving this to the script as well, this may delete some html
      # license files which is probably not intended
      find . -type d -name 'test*' -o -name 'doc' -o -name 'docs' | xargs -n1 rm -rvf
      find tarantool-sys/third_party/ -type f -name '*.htm*' | xargs -n 1 rm -rfv
      find tarantool-sys/vendor/ -type f -name '*.htm*' | xargs -n 1 rm -rfv
      find . -type d -name stress-test | xargs -n 1 rm -rvf
      find . -type d -name integration-tests | xargs -n 1 rm -rvf
      find . -type d -name 'example*' | xargs -n 1 rm -rvf
      find . -type d -name 'docsrc' | xargs -n 1 rm -rvf
      find . -name '*.md' | xargs -n 1 rm -rvf
      find http tarantool-sys vshard -type d -name .github | xargs -n 1 rm -rfv
    - |
      docker run --rm -t \
        -v $PWD:/tmp/src:rw \
        -e "SONAR_OPS=-Dsonar.python.version=3 -Dsonar.login=${SONAR} -Dsonar.projectKey=Picodata-CI -Dsonar.exclusions=**/*.mod,osv-sonar.json" \
        -e "SONAR_SCANNER_OPTS="-Xmx4096m"" \
        -e "CARGO_CLIPPY_FILE=clippy.json" \
        -u $(id -u):$(id -g) --ulimit nofile=100000:100000 --network=host \
        docker.binary.picodata.io/gamayun

build-vm-image:
  stage: test
  when: manual
  inherit:
    variables: false
  variables:
    # Not CI_COMMIT_BRANCH because CI_COMMIT_BRANCH is not available for tags
    BRANCH: $CI_COMMIT_REF_NAME
  trigger:
    project: picodata/picodata/picodata-fstek-vmbuilder
    branch: main
    strategy: depend

pack-on-tag:
  stage: pack
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
    - if: $CI_COMMIT_TAG
  variables:
    PROJECT_TARGET: "picodata"
    TYPE: "RELEASE"
    BRANCH_TARGET: $CI_COMMIT_TAG
  inherit:
    variables:
      - TYPE
      - PROJECT_TARGET
      - BRANCH_TARGET
  trigger:
    project: "picodata/devops/builder"
    strategy: depend

.deploy-docker-tmpl:
  stage: docker
  variables:
    GIT_STRATEGY: none
    PUSH_DOCKER: ""
    MULTIARCH: "true"
  rules:
    - if: $CI_COMMIT_TAG
      variables:
        PARENT_BRANCH: ${CI_COMMIT_TAG}

deploy-docker:
  extends: .deploy-docker-tmpl
  parallel:
    matrix:
      - DOCKERFILE: docker/picodata.Dockerfile
        DESTINATION: ${REGISTRY}/picodata:${CI_COMMIT_TAG}
      - DOCKERFILE: docker/picodata-distroless.Dockerfile
        DESTINATION: ${REGISTRY}/picodata:${CI_COMMIT_TAG}-distroless
  trigger:
    project: picodata/devops/picodata-in-docker
    branch: main
    strategy: depend

docker-compose:
  stage: docker
  needs:
    ["helm-image: [docker/picodata.Dockerfile, ${REGISTRY}/picodata:master]"]
  tags:
    - picodata-shell
  rules:
    - if: ($CI_COMMIT_BRANCH != $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE == "schedule")
      changes:
        compare_to: *main-branch
        paths:
          - docker/docker-compose.yml
    - if: ($CI_COMMIT_BRANCH == $MAIN_BRANCH) && ($CI_PIPELINE_SOURCE == "schedule")
      when: on_success
    - when: manual
      allow_failure: true
  script:
    - mkdir --mode=777 pico/
    - docker-compose -f docker/docker-compose.yml up -d --wait
    - sleep 30 # the cluster does not manage to assemble quickly enough
    - count=$(psql "user=admin host=127.0.0.1 port=55432 password=T0psecret sslmode=disable" -t -A -c "select count(*) from \"_pico_instance\"") && if [[ $count -eq 4 ]] ; then echo "OK"; else echo count=$count; exit 2; fi
  after_script:
    - docker-compose -f docker/docker-compose.yml rm -fsv
    - sudo rm -rf pico/

# Stages of independent stress testing in downstream pipeline
# We cannot move artefacts to the downstream pipeline, so we need to build and upload them to our repo
.upload-picodata-to-binary:
  variables:
    VER: $CI_COMMIT_SHORT_SHA
    GIT_SUBMODULE_STRATEGY: recursive
    GIT_DESCRIBE: $PICODATA_DESCRIBE
  before_script:
    # To conduct stress tests, we require the most recent picodata tag. However,
    # creating a shallow copy with $GIT_DEPTH commits might not include this tag.
    # Fetching all commits takes too much time, so we incrementally download more
    # chunks of commits until we find the required tag.
    - ./tools/get_tags.py
  script:
    - cargo build --locked --release --features webui
    - mv target/release/picodata target/release/picodata-$VER
    - curl -v --upload-file target/release/picodata-$VER $RAW_NT_REGISTRY

upload-picodata-to-binary-stress-test:
  stage: stress-test
  extends: .upload-picodata-to-binary
  rules:
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
      when: always
    - if: $CI_COMMIT_BRANCH != $MAIN_BRANCH
      when: manual
      allow_failure: true

downstream-stress-test:
  # See https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html
  stage: stress-test
  rules:
    - if: $CI_COMMIT_BRANCH == $MAIN_BRANCH
      when: always
    - if: $CI_COMMIT_BRANCH != $MAIN_BRANCH
      when: manual
      allow_failure: true
  trigger:
    project: picodata/devops/proxmox/sbroad-nt
    branch: main
    strategy: depend
  variables:
    VER: $CI_COMMIT_SHORT_SHA
  needs:
    - job: upload-picodata-to-binary-stress-test

upload-picodata-to-binary-front-deploy:
  stage: deploy
  extends: .upload-picodata-to-binary
  needs: []
  when: manual

downstream-front-deploy:
  # See https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html
  stage: deploy
  allow_failure: true
  trigger:
    project: picodata/devops/pico-servers/front
    branch: main
    strategy: depend
  variables:
    VER: $CI_COMMIT_SHORT_SHA
  needs:
    - job: upload-picodata-to-binary-front-deploy

publish-picodata-plugin:
  stage: pack
  variables:
    GIT_SUBMODULE_STRATEGY: recursive
  rules:
    - <<: *if-build-base-changes-on-dev-branch
      when: manual
      allow_failure: true
      variables:
        BASE_IMAGE_TAG: ${CI_COMMIT_SHA}
    - when: manual
      allow_failure: true
      variables:
        BASE_IMAGE_TAG: ${BASE_IMAGE_LATEST}

  script:
    - if [ -z "$CARGO_REGISTRY_TOKEN" ]; then echo "Variable CARGO_TOKEN must be available, check that branch is protected" 1>&2 && exit 1; fi
    - make publish-picodata-plugin

.test-sbroad:
  stage: test
  tags:
    - docker-k8s
  rules:
    - if: $CI_PIPELINE_SOURCE != "schedule"
  before_script:
    - pushd sbroad

stress_tests:
  parallel:
    matrix:
      - STRESS_TEST:
          - projection
          - projection_wide
          - groupby
          - insert
  tags:
    - docker-k8s
  rules:
    - if: $CI_PIPELINE_SOURCE != "schedule"
      allow_failure: true
  variables:
    TNT_HOST: tarantool
    STRESS_TEST: insert
    KUBERNETES_CPU_REQUEST: 6
    KUBERNETES_MEMORY_REQUEST: "6Gi"
  services:
    - name: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
      alias: tarantool
      pull_policy: always
  image: docker-public.binary.picodata.io/k6_tnt:latest
  stage: stress-test
  script:
    - echo "STRESS_TEST=${STRESS_TEST}"
    # wait while tnt cluster is started
    - ./tools/check_tnt.sh
    - mkdir -p sbroad/sbroad-cartridge/stress-test/$CI_COMMIT_BRANCH/${STRESS_TEST}/
    - k6 run -u 10 -d 1m -e HOST=${TNT_HOST} sbroad/sbroad-cartridge/stress-test/${STRESS_TEST}/k6.js --summary-export sbroad/sbroad-cartridge/stress-test/$CI_COMMIT_BRANCH/${STRESS_TEST}/k6_summary.json
  needs: [build-stress-image]
  artifacts:
    paths:
      - sbroad/sbroad-cartridge/stress-test/$CI_COMMIT_BRANCH/${STRESS_TEST}/k6_summary.json
    expire_in: 1 hour
    when: always

store-stress-results-for-main:
  rules:
    - if: ($CI_COMMIT_BRANCH ==  $CI_DEFAULT_BRANCH) && ($CI_PIPELINE_SOURCE != "schedule")
  stage: stress-test
  script:
    - |
      tar -czvf sbroad-main-test-results.tgz -C sbroad/sbroad-cartridge/stress-test/ .
      curl -f -H "Authorization: Basic $RAW_AUTH_RW" --upload-file sbroad-main-test-results.tgz $RAW_REGISTRY/sbroad-stress-tests/
  needs:
    - "stress_tests: [projection]"
    - "stress_tests: [projection_wide]"
    - "stress_tests: [groupby]"
    - "stress_tests: [insert]"

diff_stress-results:
  stage: stress-test
  rules:
    - if: $CI_PIPELINE_SOURCE != "schedule"
      when: manual
      allow_failure: true
  variables:
    FARCH: "sbroad-main-test-results.tgz"
  script:
    - |
      curl $RAW_REGISTRY/sbroad-stress-tests/$FARCH -o $FARCH
      set -x
      tar -xvf $FARCH -C sbroad/sbroad-cartridge/stress-test/
      cd sbroad/sbroad-cartridge/stress-test/
      find -name k6_summary.json
      for D in $(find . -name k6_summary.json -exec sh -c "dirname {} | sed 's/.*\///g'" \; ); do
        echo "Branch = $D"
        if [ "$D" != "$MAIN_BRANCH" -a -f $MAIN_BRANCH/$D/k6_summary.json -a -f $CI_COMMIT_BRANCH/$D/k6_summary.json ]; then
        tarantool compare.lua $MAIN_BRANCH/$D/k6_summary.json $CI_COMMIT_BRANCH/$D/k6_summary.json
        elif [ "$D" == "$MAIN_BRANCH" ]; then
          echo "skipped"
        else
          echo "Error: stress-test results not found!"; exit 2;
        fi
      done
  needs:
    - "stress_tests: [projection]"
    - "stress_tests: [projection_wide]"
    - "stress_tests: [groupby]"
    - "stress_tests: [insert]"
  artifacts:
    paths:
      - sbroad-cartridge/stress-test/**/**/k6_summary.json
    expire_in: 1 week
    when: always

deploy-luarocks:
  stage: deploy
  needs: []
  rules:
    - if: $CI_COMMIT_TAG
      when: manual
  before_script:
    - eval $(ssh-agent -s)
    - echo "$DEPLOY_PROD_SSH_KEY" | base64 -d | ssh-add -
    - |
      if [ "$DEPLOY_PROD_SSH_KEY" == "" ]; then
        echo "Error: Access to push in docker registry only for protected tag or branch!"
        exit 1
      fi
  script:
    - zip -r /tmp/sbroad.zip sbroad/
    - cd sbroad
    - make release_rock
    - echo "Deploying luarocks..."
    - scp -o stricthostkeychecking=no sbroad*rock luarocks@94.26.239.246:/data/nginx/www/packrepo/luarocks
    - ssh -o stricthostkeychecking=no luarocks@94.26.239.246 "luarocks-admin make_manifest /data/nginx/www/packrepo/luarocks"

clear-sccache:
  stage: test
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $SCHEDULE_TYPE == "clear_sccache"
    - when: manual
      allow_failure: true
  variables:
    GIT_STRATEGY: none
  script:
    - |
      ci-log-section start "sccache-clear" Clearing sccache ...
      rm -rf ${SCCACHE_DIR}/*
      echo "Sccache directory cleared successfully"
      ci-log-section end "sccache-clear"
  cache:
    - <<: *base_cache
      policy: push
      when: always
